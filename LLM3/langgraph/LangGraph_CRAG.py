import os
import warnings
warnings.filterwarnings("ignore")

from typing import List, Literal
from typing_extensions import TypedDict
from dotenv import load_dotenv

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.document_loaders import DirectoryLoader, TextLoader

# LangGraph ê´€ë ¨ ì„í¬íŠ¸
from langgraph.graph import StateGraph, START, END

# í™˜ê²½ì„¤ì •
load_dotenv()

if not os.environ.get('OPENAI_API_KEY'):
    raise ValueError('key check...')

class CGRAState(TypedDict):
    question : str
    documents : List[Document]
    web_search_needed : str # ì›¹ê²€ìƒ‰ ì—¬ë¶€(yes/no)
    context : str
    answer : str
    grade_results : List[str] # ê° ë¬¸ì„œì˜ í‰ê°€ê²°ê³¼

# Step 1 ë¬¸ì„œ
path = 'C:/Users/playdata2/Desktop/LLM/LLM3/advenced/sample_docs'
loader = DirectoryLoader(
    path = path,
    glob = '**/*.txt',
    loader_cls = TextLoader,
    loader_kwargs = {'encoding': 'utf-8'},
)
docs = loader.load()

# Step 2 í…ìŠ¤íŠ¸ ë¶„í•  (ì²­í¬)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300, chunk_overlap=50
)
doc_splits = text_splitter.split_documents(docs)

# Step 3 ì„ë² ë”© ë° VectorDB
vectorstore = Chroma.from_documents(
    documents=doc_splits,
    collection_name='crag_collection',
    embedding=OpenAIEmbeddings(model='text-emb')
)

# Step 4 ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
retriever = vectorstore.as_retriever(search_kwars={'k':3})

print(f'{len(doc_splits)}ê°œ ì²­í¬ë¡œ VectorDB êµ¬ì¶• ì™„ë£Œ')

# ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ë¥¼ ìœ„í•œ Grader ì •ì˜
from pydantic import BaseModel, Field
class GradeDocuments(BaseModel):
    '''ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼ë¥¼ ìœ„í•œ pydantic ëª¨ë¸'''
    binary_score:str = Field(description="ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆìœ¼ë©´ 'yes', ì—†ìœ¼ë©´ 'no'")

# llm
grader_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)
structured_grader = grader_llm.with_structured_output(GradeDocuments)
grade_prompt = ChatPromptTemplate.from_messages([
    ('system', '''ë‹¹ì‹ ì€ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ ì…ë‹ˆë‹¤.
     
     í‰ê°€ê¸°ì¤€:
     - ë¬¸ì„œê°€ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë‚˜ ì˜ë¯¸ì™€ ì—°ê´€ë˜ì–´ ìˆë‹¤ë©´ 'ê´€ë ¨ìˆìŒ'ìœ¼ë¡œ í‰ê°€
     - ë‹µë³€ì— ë„ì›€ì´ ë  ê°€ëŠ¥ì„±ì´ ì¡°ê¸ˆì´ë¼ë„ ìˆë‹¤ë©´ 'ê´€ë ¨ìˆìŒ'
     - ì™„ì „íˆ ë¬´ê´€í•œ ë‚´ìš©ì´ë©´ 'ê´€ë ¨ì—†ìŒ'

     ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ì§€ ë§ê³ , ì•½ê°„ì˜ ì—°ê´€ì„±ì´ë¼ë„ ìˆìœ¼ë©´ 'yes'ë¥¼ ë°˜í™˜í•˜ì„¸ìš”
     '''),
     ('human', '''ì§ˆë¬¸:{question}
      
      ë¬¸ì„œë‚´ìš©:
      {document}

      ì´ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆê¹Œ? 'yes' ë˜ëŠ” 'no'ë¡œë§Œ ë‹µí•˜ì„¸ìš”
      ''')
])

document_grader = grade_prompt | structured_grader

def retrieve_node(state: CGRAState) -> dict:
    '''ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ'''
    question = state['question']
    documents = retriever.invoke(question)
    return {
        'documents': documents,
        'question': question
    }


def grade_documents_node(state:CGRAState) -> dict:
    '''ë¬¸ì„œê´€ë ¨ì„± í‰ê°€ ë…¸ë“œ
    ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„± ì—¬ë¶€ë¥¼ llm í‰ê°€ 
    ê´€ë ¨ì—†ìœ¼ë©´ ì›¹ ê²€ìƒ‰ í”Œë˜ê·¸ë¥¼ í™œì„±
    '''
    question = state['question']
    documents = state['documents']
    filtered_docs, grade_results = [],[]
    for i, doc in enumerate(documents,1):
        # ê° ë¬¸ì„œì˜ ê´€ë ¨ì„± í‰ê°€
        score = document_grader.invoke({
            'question' : question,
            'document' : doc.page_content
        })
        grade = score.binary_score.lower()
        if grade == 'yes':
            filtered_docs.append(doc)
            grade_results.append("relevant")
        else:
            grade_results.append("not_relevant")
     # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì›¹ ê²€ìƒ‰ í•„ìš”
    if len(filtered_docs) == 0:
        web_search_needed = "Yes"
        print("   ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ â†’ ì›¹ ê²€ìƒ‰ í•„ìš”!")
    else:
        web_search_needed = "No"
        print(f"  {len(filtered_docs)}ê°œ ê´€ë ¨ ë¬¸ì„œ í™•ë³´!")
    
    return {
        "filtered_documents": filtered_docs,
        "web_search_needed": web_search_needed,
        "grade_results": grade_results
    }

def web_search_node(state: CGRAState) -> dict:
    """
    ì›¹ ê²€ìƒ‰ ë…¸ë“œ (ì‹œë®¬ë ˆì´ì…˜)
    
    ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Tavily APIë‚˜ ë‹¤ë¥¸ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ì—¬ê¸°ì„œëŠ” í•™ìŠµ ëª©ì ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.
    """
    print("\n   [WEB SEARCH ë…¸ë“œ] ì™¸ë¶€ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
    
    question = state["question"]
    
    # ì›¹ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” Tavily API ë“± ì‚¬ìš©)
    # ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ:
    # from langchain_community.tools.tavily_search import TavilySearchResults
    # web_search = TavilySearchResults(k=3)
    # web_results = web_search.invoke({"query": question})
    
    # ì‹œë®¬ë ˆì´ì…˜ëœ ì›¹ ê²€ìƒ‰ ê²°ê³¼
    simulated_web_results = f"""
    [ì›¹ ê²€ìƒ‰ ê²°ê³¼ - ì‹œë®¬ë ˆì´ì…˜]
    
    ì§ˆë¬¸ '{question}'ì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼:
    
    1. LLM(Large Language Model) ê´€ë ¨ ìµœì‹  ì •ë³´:
       - LLMì€ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ í˜ì‹ ì ì¸ ë°œì „ì„ ì´ë£¨ê³  ìˆìŠµë‹ˆë‹¤.
       - OpenAI, Anthropic, Google ë“±ì´ ì£¼ìš” ì œê³µìì…ë‹ˆë‹¤.
       - RAG, Fine-tuning, Prompt Engineeringì´ ì£¼ìš” í™œìš© ê¸°ë²•ì…ë‹ˆë‹¤.
    
    2. AI ì—ì´ì „íŠ¸ íŠ¸ë Œë“œ:
       - ììœ¨ì ì¸ AI ì—ì´ì „íŠ¸ê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.
       - LangGraph, AutoGPT ë“±ì´ ëŒ€í‘œì ì¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
       - ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì´ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    ì¶œì²˜: ì‹œë®¬ë ˆì´ì…˜ëœ ì›¹ ê²€ìƒ‰ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Tavily API ì‚¬ìš©)
    """
    
    # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ Document í˜•íƒœë¡œ ë³€í™˜
    web_doc = Document(
        page_content=simulated_web_results,
        metadata={"source": "web_search", "type": "external"}
    )
    
    # ê¸°ì¡´ í•„í„°ë§ëœ ë¬¸ì„œì— ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
    filtered_docs = state.get("filtered_documents", [])
    filtered_docs.append(web_doc)
    
    print("   ì›¹ ê²€ìƒ‰ ì™„ë£Œ! ê²°ê³¼ê°€ ë¬¸ì„œì— ì¶”ê°€ë¨")
    
    return {
        "filtered_documents": filtered_docs
    }


def generate_node(state: CGRAState) -> dict:
    """
    ë‹µë³€ ìƒì„± ë…¸ë“œ
    í•„í„°ë§ëœ ë¬¸ì„œ(ë‚´ë¶€ ë¬¸ì„œ + ì›¹ ê²€ìƒ‰ ê²°ê³¼)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("\n   ğŸ’¬ [GENERATE ë…¸ë“œ] ë‹µë³€ ìƒì„± ì¤‘...")
    
    question = state["question"]
    filtered_documents = state["filtered_documents"]
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = "\n\n---\n\n".join([doc.page_content for doc in filtered_documents])
    
    # ë‹µë³€ ìƒì„± LLM
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ê·œì¹™:
1. ì œê³µëœ ë¬¸ë§¥ ë‚´ì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
2. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  êµ¬ì¡°í™”ë˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
3. ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ í¬í•¨ëœ ê²½ìš°, í•´ë‹¹ ì •ë³´ë„ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”.
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”."""),
        ("human", """ë¬¸ë§¥(Context):
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:""")
    ])
    
    chain = prompt | llm | StrOutputParser()
    answer = chain.invoke({"context": context, "question": question})
    
    print("   ë‹µë³€ ìƒì„± ì™„ë£Œ!")
    
    return {
        "context": context,
        "answer": answer
    }


# ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜ ì •ì˜

def decide_to_generate(state: CGRAState) -> Literal["generate", "web_search"]:
    """
    ë¬¸ì„œ í‰ê°€ ê²°ê³¼ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    
    - ê´€ë ¨ ë¬¸ì„œê°€ ìˆìœ¼ë©´ â†’ generate (ë‹µë³€ ìƒì„±)
    - ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ â†’ web_search (ì›¹ ê²€ìƒ‰)
    
    Returns:
        "generate" ë˜ëŠ” "web_search"
    """
    print("\n   [DECISION] ë‹¤ìŒ ë‹¨ê³„ ê²°ì • ì¤‘...")
    
    web_search_needed = state["web_search_needed"]
    
    if web_search_needed == "Yes":
        print("   ê²°ì •: ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì´ë™")
        return "web_search"
    else:
        print("   ê²°ì •: ë‹µë³€ ìƒì„±ìœ¼ë¡œ ì´ë™")
        return "generate"


print("ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!")

print("\n CRAG StateGraph êµ¬ì„± ë° ì»´íŒŒì¼ ì¤‘...")

# StateGraph ìƒì„±
workflow = StateGraph(CGRAState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("grade_documents", grade_documents_node)
workflow.add_node("web_search", web_search_node)
workflow.add_node("generate", generate_node)

# ì—£ì§€ ì¶”ê°€
# START -> retrieve -> grade_documents
workflow.add_edge(START, "retrieve")
workflow.add_edge("retrieve", "grade_documents")

# ì¡°ê±´ë¶€ ì—£ì§€: grade_documents ì´í›„ ë¶„ê¸°
# - ê´€ë ¨ ë¬¸ì„œ ìˆìŒ â†’ generate
# - ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ â†’ web_search
workflow.add_conditional_edges(
    "grade_documents",      # ì‹œì‘ ë…¸ë“œ
    decide_to_generate,     # ì¡°ê±´ í•¨ìˆ˜
    {
        "generate": "generate",      # "generate" ë°˜í™˜ ì‹œ
        "web_search": "web_search"   # "web_search" ë°˜í™˜ ì‹œ
    }
)

# web_search ì´í›„ generateë¡œ ì´ë™
workflow.add_edge("web_search", "generate")

# generate ì´í›„ ì¢…ë£Œ
workflow.add_edge("generate", END)

# ê·¸ë˜í”„ ì»´íŒŒì¼
app = workflow.compile()


# í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
test_cases = [
    {
        "question": "LangGraphì˜ í•µì‹¬ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "expected": "ë‚´ë¶€ ë¬¸ì„œì—ì„œ ë‹µë³€ ê°€ëŠ¥ â†’ ì›¹ ê²€ìƒ‰ ë¶ˆí•„ìš”"
    },
    {
        "question": "CRAG íŒ¨í„´ì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "expected": "ë‚´ë¶€ ë¬¸ì„œì—ì„œ ë‹µë³€ ê°€ëŠ¥ â†’ ì›¹ ê²€ìƒ‰ ë¶ˆí•„ìš”"
    },
    {
        "question": "ìµœì‹  GPT-5 ëª¨ë¸ì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "expected": "ë‚´ë¶€ ë¬¸ì„œì— ì—†ìŒ â†’ ì›¹ ê²€ìƒ‰ í•„ìš”"
    }
]

for i, test in enumerate(test_cases, 1):
    print(f"\n{'â”' * 70}")
    print(f" í…ŒìŠ¤íŠ¸ {i}: {test['question']}")
    print(f"   ì˜ˆìƒ ì‹œë‚˜ë¦¬ì˜¤: {test['expected']}")
    print(f"{'â”' * 70}")
    
    # ì´ˆê¸° ìƒíƒœ
    initial_state = {
        "question": test["question"],
        "documents": [],
        "filtered_documents": [],
        "web_search_needed": "No",
        "context": "",
        "answer": "",
        "grade_results": []
    }
    
    # ê·¸ë˜í”„ ì‹¤í–‰
    print("\n CRAG ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...")
    
    final_state = None
    for output in app.stream(initial_state):
        for node_name, node_output in output.items():
            print(f"   ë…¸ë“œ '{node_name}' ì‹¤í–‰ ì™„ë£Œ")
        final_state = output
    
    # ê²°ê³¼ ì¶œë ¥
    if "generate" in final_state:
        answer = final_state["generate"]["answer"]
    else:
        answer = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    print(f"\n ìµœì¢… ë‹µë³€:\n{answer}")