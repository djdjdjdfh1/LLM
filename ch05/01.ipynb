{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a31d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bow  Bag of Words  CountVectorizer\n",
    "# 문서를 고정된 길이의 벡터로 변환\n",
    "# 문서 - 단어행렬\n",
    "# 장점 : 간단하고 빠름,\n",
    "# 단점 : 단어순서 손실, 희소성, 의미적 유사성 무시\n",
    "\n",
    "# tf-idf TfidfVectorizer\n",
    "# 모든 문서에서 자주등장하는 단어의 영향을 줄이고 문서 특이 단어를 강조\n",
    "\n",
    "# multinomal Naive Bayes  확률 모델\n",
    "# LogisticRegression : 다중클래스  회귀기반 분류\n",
    "\n",
    "# RidgeClassifer : 회귀 기반 분류 L2규제\n",
    "\n",
    "# N-gram : 단점 차원폭발에 주의 (정규화/차원 축소 고려)\n",
    "\n",
    "# Kolnpy Okt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# scikit-learn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#분류모델\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc956d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories =  ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "# # data load\n",
    "newsgroups_train =  fetch_20newsgroups(subset='train'\n",
    "                    ,remove = ('headers','footers','quotes')\n",
    "                    ,categories=categories\n",
    "                   )\n",
    "newsgroups_test =  fetch_20newsgroups(subset='test'\n",
    "                    ,remove = ('headers','footers','quotes')\n",
    "                    ,categories=categories\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e4b5da",
   "metadata": {},
   "source": [
    "categories =  ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "- alt.atheism\t무신론, 종교 비판 토론\t종교의 존재, 신의 존재 유무, 종교적 주장 반박, 철학적 논쟁 등\n",
    "- talk.religion.misc\t일반 종교 토론 (기타 잡담 포함)\t기독교, 불교, 이슬람 등 다양한 종교 관련 이야기, 개인 경험, 신념 공유 등\n",
    "- comp.graphics\t컴퓨터 그래픽스, 이미지 처리\t3D 렌더링, 이미지 파일 포맷, 그래픽 소프트웨어 사용법, OpenGL 등 기술 관련 토론\n",
    "- sci.space 우주 과학, 천문학 로켓, NASA, 행성 탐사, 외계 생명 가능성, 우주 물리학 관련 토론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42606dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 제거\n",
    "def filter_categories(dataset, categories):\n",
    "    target_names = dataset.target_names\n",
    "    selected_idx = [ target_names.index(c) for c in categories  ]\n",
    "    #필터링\n",
    "    data_filtered, target_filtered = [], []\n",
    "    for text,label in zip(dataset.data, dataset.target):\n",
    "        if label in selected_idx:\n",
    "            new_label = selected_idx.index(label)  # 라벨 재 정렬\n",
    "            data_filtered.append(text) ; target_filtered.append( new_label  )\n",
    "    return data_filtered,target_filtered,categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d15ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_target, target_names = filter_categories(newsgroups_train,categories)\n",
    "test_data, test_target, _ = filter_categories(newsgroups_test,categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb8bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해더 푸터 인용문 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 헤더 제거\n",
    "    text = re.sub(r'^From:.*\\n', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^Subject:.*\\n', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 풋터 제거\n",
    "    text = re.sub(r'\\n--\\n.*$', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # 인용문 제거\n",
    "    text = re.sub(r'(^|\\n)[>|:].*', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de6c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [ clean_text(t) for t in train_data]\n",
    "test_data = [ clean_text(t) for t in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(train_target), len(test_data), len(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff05c2",
   "metadata": {},
   "source": [
    "- 멀티노멀 나이즈베이즈\n",
    "- 문서에 포함된 단어들의 출현 횟수를 기반으로 해서 그 문서가 어떤 주제에 속할지 확률적\n",
    "- 스팸필터링, 뉴스기사 카테고리, 감성분석\n",
    "\n",
    "- 베이즈정리 확률 이론 - 조건부 확률\n",
    "- 단어A가 나왔을때 이 문서가 스팸 B 일 확률은 얼마\n",
    "\n",
    "$P(\\text{스팸} | \\text{단어들}) = \\frac{P(\\text{단어들} | \\text{스팸}) \\cdot P(\\text{스팸})}{P(\\text{단어들})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932262f",
   "metadata": {},
   "source": [
    "- 나이브 Naive  : 순진한 가정\n",
    "    - 가정 : 문서안의 모든 단어는 서로 독립적\n",
    "    - 현실 : 스펨에 자주 나오는 단어들은 서로 독립적이지 않다\n",
    "    - 실제 : 이러한 가정은 계산량을 빠르게 하고 단순하지만 정확도가 어느정도 나온다\n",
    "- 멀티노멀 : 다항 분포\n",
    "    - 의미 : 단어의 출현 횟수를 중요\n",
    "    - 횟수를 세는 멀티노미얼 방식이 NLP 잘 맞는다\n",
    "    - 모델은 단어의 빈도수 통계\n",
    "    - 스펨메일 통계 spem\n",
    "        - free : 150\n",
    "        - money : 100\n",
    "        - viagra : 50\n",
    "        - report : 5\n",
    "    - 정상메일 Ham 통계\n",
    "        - report:80\n",
    "        - metting : 60\n",
    "        - free : 10\n",
    "    - 이러한 통계를 바탕으로 이 카테고리에서 특정 단어가 나올 확률 P('free'|스펨) 을 모두 계산\n",
    "    - \"Free money meeting\"\n",
    "        - 스펨??\n",
    "            - 기본스펨확률 x 스팸일때 free가 나올 확률 x 스팸일때 money 나올 확률 x 스팸일때 meeting 나올 확률\n",
    "        - 정상\n",
    "            - 기본정상확률 x 정상일때...   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d64ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk tokenizer stemer\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6623521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_df : 단어의 빈도가 최소 5개의 문서에 등장  - 노이즈 감소\n",
    "# max_df : 50% 너무 흔한 단어는 제거\n",
    "cv = CountVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_cv = cv.fit_transform(train_data)\n",
    "x_test_cv = cv.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f66f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b9c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cv[0].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW 기반\n",
    "# 텍스트 분류의 강력한 baseline 희소데이터에 강함\n",
    "# 모델선택\n",
    "nb = MultinomialNB()\n",
    "# 학습용 데이터, 벡터 데이터\n",
    "nb.fit(x_train_cv, train_target)\n",
    "print(nb.score(x_train_cv, train_target), nb.score(x_test_cv, test_target))\n",
    "# 분류 리포트\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_nb = nb.predict(x_test_cv)\n",
    "print( classification_report(test_target, y_pred_nb, target_names=categories) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + MNB + LogisticRegression\n",
    "# TF-IDF로 중요단어 강조, 선형모델과 자주 사용  BOW 대비 흔한 단어 영향 감소\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfid = tfidf.fit_transform(train_data)\n",
    "x_test_tfid = tfidf.transform(test_data)\n",
    "\n",
    "# NB + tf-idf\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(x_train_tfid, train_target)\n",
    "print(nb_tfidf.score(x_train_tfid, train_target), nb_tfidf.score(x_test_tfid, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b63fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_tfid, train_target, test_size=0.2, stratify=train_target, random_state=42)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(x_train, y_train)\n",
    "print(lr.score(x_train, y_train), lr.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과적합 해결을 위한 규제\n",
    "rc = RidgeClassifier(alpha=10)\n",
    "rc.fit(x_train, y_train)\n",
    "print(rc.score(x_train, y_train), rc.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d595ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 규제   L1 Logistic(Lasso와 유사)\n",
    "# 일부 계수를 0으로 만들어서 특성 선택을 수행..\n",
    "l1_lr = LogisticRegression(penalty='l1', max_iter=1000, solver='liblinear')\n",
    "l1_lr.fit(x_train, y_train)\n",
    "print(l1_lr.score(x_train, y_train), l1_lr.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트리모델 + tfidf\n",
    "tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "gb = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf14fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_data, train_target, stratify=train_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fdf613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "# RegexpTokenizer + stopwords + PorterStemmer\n",
    "english_stops = set(stopwords.words('english'))\n",
    "regtok = RegexpTokenizer(r\"[\\w]{3,}\")\n",
    "regtok.tokenize('i love you')\n",
    "def custom_tokenizer(text):\n",
    "    toks = regtok.tokenize(text.lower())\n",
    "    toks = [t for t in toks if t not in english_stops]\n",
    "    tokes = [PorterStemmer().stem(t) for t in toks]\n",
    "    return tokes\n",
    "tfidf_custom = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfidf_c = tfidf_custom.fit_transform(x_train)\n",
    "x_test_tfidf_c = tfidf_custom.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd326e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_c = LogisticRegression(max_iter=1000)\n",
    "lr_c.fit(x_train_tfidf_c, y_train)\n",
    "print( lr_c.score(x_train_tfidf_c, y_train), lr_c.score(x_test_tfidf_c, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef539b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram 실험 1,2  1,3\n",
    "# 성능향상 기대  연속된 단어패턴 포착\n",
    "tfidf_12 = TfidfVectorizer(token_pattern = r\"[\\w]){3,}\", \n",
    "                           stop_words=stopwords.words('english'),\n",
    "                           ngram_range=(1,2),\n",
    "                           min_df=2, max_df=0.5,\n",
    "                           max_features=2000\n",
    "                           )\n",
    "x_train_12 = tfidf_12.fit_transform(x_train)\n",
    "x_test_12 = tfidf_12.transform(x_test)\n",
    "\n",
    "lr_c = LogisticRegression(max_iter=1000)\n",
    "lr_c.fit(x_train_12, y_train)\n",
    "print( lr_c.score(x_train_12, y_train), lr_c.score(x_test_12, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6919a8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.26</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.24</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이 정도면 볼만하다고 할 수 있음!</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.22</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>재미있다</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.20</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date  \\\n",
       "0                             돈 들인건 티가 나지만 보는 내내 하품만       1  2018.10.29   \n",
       "1       몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.      10  2018.10.26   \n",
       "2  이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...       8  2018.10.24   \n",
       "3                                이 정도면 볼만하다고 할 수 있음!       8  2018.10.22   \n",
       "4                                               재미있다      10  2018.10.20   \n",
       "\n",
       "    title  \n",
       "0  인피니티 워  \n",
       "1  인피니티 워  \n",
       "2  인피니티 워  \n",
       "3  인피니티 워  \n",
       "4  인피니티 워  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한국어 처리  konlpy\n",
    "# 품사기반 태깅 tokenizer   Noun, Verb, Adjective\n",
    "# 데이터 로딩\n",
    "import pandas as pd\n",
    "url = \"https://drive.google.com/uc?id=1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c86fc2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['인피니티 워', '라라랜드', '곤지암', '신과함께', '범죄도시', '택시운전사', '코코'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "019d6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.review, df.title, stratify=df.title, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fc99bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt \n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1ee0f7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7565365025466892 0.6899830220713074\n"
     ]
    }
   ],
   "source": [
    "# simple version\n",
    "tfidf = TfidfVectorizer(tokenizer=okt.nouns, max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf.transform(x_test)\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(x_train_tfidf, y_train)\n",
    "print(clf.score(x_train_tfidf, y_train), clf.score(x_test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f3664128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\deep\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782258064516129 0.7137521222410866\n"
     ]
    }
   ],
   "source": [
    "# simple version\n",
    "okt = Okt()\n",
    "def custom_tokenizer(text):\n",
    "    target = ['Noun', 'Verb', 'Adjective']\n",
    "    return [w for w,tag in okt.pos(text, norm=True, stem=True) if tag in target]\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf.transform(x_test)\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(x_train_tfidf, y_train)\n",
    "print(clf.score(x_train_tfidf, y_train), clf.score(x_test_tfidf, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
