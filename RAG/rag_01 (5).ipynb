{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJcgkfBAn_5l"
      },
      "outputs": [],
      "source": [
        "# RAG\n",
        "# 우리회사의 복지제도는?\n",
        "# LLM은 학습데이터에 없는 최신/특정 정보를 모름\n",
        "\n",
        "# RAG 해결책  (Tetrieval Augmented Generation) = 검색 + 생성\n",
        "# 1. 회사 문서에서 관련 정보 검색\n",
        "# 2. 검색된 정보를 LLM에게 컨텍스트로 제공\n",
        "# 3. llm이 컨텍스 기반으로 정확한 답변 생성\n",
        "\n",
        "# [준비 단계]\n",
        "# 문서들 ->청크분할->벡터변환->벡터DB저장\n",
        "\n",
        "#[쿼리 단계]\n",
        "# 질문->벡터변환->유사도검색->상위 k개 선택 -> 컨텍스트 +질문->LLM->답변"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2vwal1KoElJ"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnxUXQlOoHa-"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document, VectorStoreIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pWVppwfPqUEP",
        "outputId": "1bc2d5e8-816d-44d9-e0c5-5ca5a7a030e8"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('OPENAI_API_KEY')[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHrB1mYyohG9",
        "outputId": "86725999-9eed-463b-d347-db022868c52c"
      },
      "outputs": [],
      "source": [
        "# 1 문서 준비\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "document = [\n",
        "    Document(text='대한민국의 수도는 서울입니다.'),\n",
        "    Document(text=\"프랑스의 수도는 파리 입니다.\")\n",
        "]\n",
        "# 2 인덱스 생성(자동으로 벡터화)\n",
        "# 각 청크를 openai api 로 벡터화\n",
        "# 인메모리방식으로 벡터 스토어에 저장\n",
        "index = VectorStoreIndex.from_documents(document)\n",
        "\n",
        "# 3 쿼리 엔진 생성\n",
        "query_engine = index.as_query_engine(similarity_top_k=1)\n",
        "\n",
        "# 4 쿼리 실행\n",
        "response = query_engine.query(\"대한민국의 수도는 어디입니까?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR8lvleTvvwi"
      },
      "outputs": [],
      "source": [
        "# 청크 : 문서검색의 최소단위 모델이 한번에 처리할수 있는 길이로 잘라낸 텍스트\n",
        "# 모델 입력 길이 제한, 문서가 길면 한번에 처리 할 수 없어서 청크로 나눠 처리\n",
        "# 벡터 DB에서 문서전체가 아니라 청크단위로 벡터화\n",
        "# 질문과 유사한 작은 단위를 찾아 답변을 생성\n",
        "# 전체문서를 이해하는 대신 청크별로 처리해서  중요한 부분에 집중\n",
        "\n",
        "# 작을수록 : 정확한 검색, 많은 api 호출\n",
        "# 클수록 : 넓은 컨텍스트, 적은 api 호출\n",
        "from llama_index.core import Settings\n",
        "Settings.chunk_size = 512  # 기본값\n",
        "# Settings.chunk_overlap = 128  # 기본값\n",
        "Settings.chunk_overlap = 50 # 청크 간 겹침\n",
        "\n",
        "# 유사도 임계값 설정\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=2,  # 유사도 상위 2\n",
        "    node_postprocessors=[\n",
        "        SimilarityPostprocessor(similarity_cutoff=0.7)  # 유사도 0.7미만의 문서는 제외 (노이즈 제거)\n",
        "    ]\n",
        ")\n",
        "# 배치 처리\n",
        "Settings.embed_batch_size = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1RfDsvxzQLL"
      },
      "source": [
        "한국어 데이터로 RAG 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE7hlm8RwYzW"
      },
      "outputs": [],
      "source": [
        "# 한국어 데이터로 RAG 구현\n",
        "from llama_index.core import Document,VectorStoreIndex,Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5NhjjvhyqJq"
      },
      "outputs": [],
      "source": [
        "#. 1. LLM, 임베딩 모델 설정\n",
        "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xMntSDOy8uZ"
      },
      "outputs": [],
      "source": [
        "# 2. 문서 준비\n",
        "documents = [\n",
        "    Document(\n",
        "        text=\"김치는 한국의 대표적인 발효 음식입니다. 배추에 고춧가루, 마늘, 생강 등을 넣어 만듭니다.\",\n",
        "        metadata={\"source\": \"한국 음식 백과\", \"category\": \"반찬\"}\n",
        "    ),\n",
        "    Document(\n",
        "        text=\"비빔밥은 밥 위에 여러 가지 나물과 고기, 계란을 올려 고추장과 섞어 먹는 음식입니다.\",\n",
        "        metadata={\"source\": \"한국 음식 백과\", \"category\": \"밥 요리\"}\n",
        "    ),\n",
        "    Document(\n",
        "        text=\"불고기는 양념한 소고기를 구워 먹는 한국의 전통 음식입니다. 달콤하고 짭짤한 맛이 특징입니다.\",\n",
        "        metadata={\"source\": \"한국 음식 백과\", \"category\": \"고기 요리\"}\n",
        "    ),\n",
        "    Document(\n",
        "        text=\"떡볶이는 가래떡에 고추장 양념을 넣어 볶은 한국의 길거리 음식입니다. 달콤하고 매운 맛이 특징입니다.\",\n",
        "        metadata={\"source\": \"한국 음식 백과\", \"category\": \"분식\"}\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyFWwuT_zN-P"
      },
      "outputs": [],
      "source": [
        "# 3. 벡터 인덱스 생성\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QiMB3RSzdlh"
      },
      "outputs": [],
      "source": [
        "# 4. 쿼리 엔진 생성\n",
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=2,\n",
        "    # node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtgD88pdzvdS",
        "outputId": "b571af2e-71a9-43dd-fb47-f1417a75d0d5"
      },
      "outputs": [],
      "source": [
        "# 5. 질문하기\n",
        "questions = [\n",
        "    '김치는 어떤 음식인가요?',\n",
        "    '비빔밥을 어떻게 먹나요?',\n",
        "    '한국의 고기 요리에는 뭐가 있나요?'\n",
        "]\n",
        "for q in questions:\n",
        "  response = query_engine.query(q)\n",
        "  print(f'질문:{q} 답변 :{response}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_e32Wn_3rKs"
      },
      "source": [
        "LLM 캐시"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TogPfVDZ0N1R",
        "outputId": "278dc78b-3d81-4350-b0cf-70c8aee3ccc5"
      },
      "outputs": [],
      "source": [
        "# 동일한 질문을 반복하면\n",
        "from openai import OpenAI\n",
        "import openai\n",
        "import time\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI()\n",
        "question = '대한민국의 수도는'\n",
        "\n",
        "start = time.time()\n",
        "for i in range(100):\n",
        "  response = client.chat.completions.create(\n",
        "      model = 'gpt-4o-mini',\n",
        "      messages = [{'role':'user', 'content':question}],\n",
        "      temperature = 0\n",
        "  )\n",
        "  answer = response.choices[0].message.content\n",
        "elapsed_time = time.time() - start\n",
        "print(f'elapsed_time:{elapsed_time}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaU-5rrH5Ihh",
        "outputId": "4a3e3f38-c1d7-4cb9-d3de-5d730fa8f993"
      },
      "outputs": [],
      "source": [
        "# 캐시.. 완전 일치 캐시(Exact Match Cache)\n",
        "# 동일한 입력 ->저장된 응답 반환\n",
        "# 장점 : 구현이 간단하고, 100% 정확\n",
        "# 단점 : 완전히 같아야만 작동\n",
        "\n",
        "# 대한민국의 수도는?  캐시 히트\n",
        "# 대한민국 수도는?    캐시 미스(다른 문자열)\n",
        "# 한국의 수도는?      캐시 미스\n",
        "\n",
        "cache = {}\n",
        "response = []\n",
        "start = time.time()\n",
        "for i in range(100):\n",
        "  if question in cache:\n",
        "    asnwer = cache[question]\n",
        "  else:\n",
        "    response = client.chat.completions.create(\n",
        "      model = 'gpt-4o-mini',\n",
        "      messages = [{'role':'user', 'content':question}],\n",
        "      temperature = 0\n",
        "    )\n",
        "    answer = response.choices[0].message.content\n",
        "    cache[question] = answer\n",
        "elapsed_time = time.time() - start\n",
        "print(f'elapsed time: {elapsed_time}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U3He8au--TS"
      },
      "outputs": [],
      "source": [
        "# 의미적 캐시 (Semantic Cache)\n",
        "# 으미가 비슷한 입력->저장된 응답 반환\n",
        "# 1. 유사한 프롬프트 검색\n",
        "# 2. 유사도 확인  (특정 임계값을 지정해서 그 값에따라서 답변 채택 종료)\n",
        "# 3. 비슷한게 없으면 llm 호출\n",
        "\n",
        "# 장점 :\n",
        "# 높은 히트율\n",
        "# 다양한 표현허용 유연함\n",
        "# 비용 절감\n",
        "\n",
        "# 단점\n",
        "# 약간 느림\n",
        "# 벡터 DB 필요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYIrXPjbAPqj"
      },
      "outputs": [],
      "source": [
        "%pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izCNiX4b_uQm"
      },
      "outputs": [],
      "source": [
        "# 벡터스토어  -> DB\n",
        "# 문서나 텍스트를 벡터로 변환한후에 저장 -> 유사도 기반 검색 기능\n",
        "import chromadb\n",
        "# 클라이언트 생성\n",
        "client = chromadb.Client()\n",
        "# 컬렉션 생성\n",
        "collection = client.create_collection(\"my_collection2\")\n",
        "# 문서와 임베딩 준비\n",
        "texts = [\n",
        "    '대한민국의 수도는 서울입니다.',\n",
        "    '프랑스의 수도는 파리 입니다.',\n",
        "    '서울은 한국의 정치,경제 중심지 입니다.'\n",
        "]\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(texts).tolist()\n",
        "embeddings  # (3,384)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBKTcu60CHuF"
      },
      "outputs": [],
      "source": [
        "# 문서추가\n",
        "ids = ['doc1','doc2','doc3']\n",
        "collection.add(\n",
        "    ids=ids,\n",
        "    documents=texts,\n",
        "    embeddings=embeddings\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zsA17DoCh7g",
        "outputId": "584933d6-eb42-4bf1-f48c-b7d58bd8a87d"
      },
      "outputs": [],
      "source": [
        "# 유사도 검색\n",
        "query = '조선의 수도는 어디인가요?'\n",
        "query_embedding = model.encode([query]).tolist()\n",
        "results = collection.query(\n",
        "    query_embeddings=query_embedding,\n",
        "    n_results=1\n",
        ")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ9qUvjZEShX"
      },
      "outputs": [],
      "source": [
        "# 다층 캐시 전략\n",
        "# 메모리(완전일치) - 미스 벡터DB(의미적) - 미스 LLM호출\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpFlXB1NElqE"
      },
      "outputs": [],
      "source": [
        "# 1. 완전 일치\n",
        "class SimpleCache:\n",
        "  def __init__(self):\n",
        "    self.cache = {}  # 딕셔너리\n",
        "    self.hits = 0\n",
        "    self.misses = 0\n",
        "  def get(self, key):\n",
        "    if key in self.cache:\n",
        "      self.hits += 1\n",
        "      return self.cache[key]\n",
        "    self.misses += 1\n",
        "    return None\n",
        "  def set(self,key,value):\n",
        "    self.cache[key] = value\n",
        "  def state(self):\n",
        "    total = self.hits + self.misses\n",
        "    hit_rate = self.hits / total*100 if total > 0 else 0\n",
        "    return{\n",
        "        'hits':self.hits,\n",
        "        'misses':self.misses,\n",
        "        'hit_rate':hit_rate\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmq57EwlGtTJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "# openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI(api_key = userdata.get('OPENAI_API_KEY'))\n",
        "def call_llm(question):\n",
        "  response = client.chat.completions.create(\n",
        "      model = 'gpt-5-nano',\n",
        "      messages = [{'role':'user', 'content':question}],\n",
        "      temperature = 0\n",
        "    )\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSlgRsUtF6tk",
        "outputId": "d6f2ab9f-4d76-449a-ca05-e4539329986e"
      },
      "outputs": [],
      "source": [
        "cache = SimpleCache()\n",
        "questions = [\n",
        "    '대한민국의 수도는?',\n",
        "    '대한민국의 수도는?',  # 캐시 히트\n",
        "    '한국의 수도는?',      # 캐시 미스(다른 문자열)\n",
        "]\n",
        "for q in questions:\n",
        "  cached = cache.get(q)\n",
        "  if cached:\n",
        "    print(f'캐시 : {cached}')\n",
        "  else:\n",
        "    response = call_llm(q)\n",
        "    cache.set(q,response)\n",
        "    print(f' llm : {response}')\n",
        "print(cache.state())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ7fI-mrH0vL"
      },
      "outputs": [],
      "source": [
        "# 2. 의미적 유사성 - 벡터 DB chromadb\n",
        "import chromadb\n",
        "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
        "class SemanticCache:\n",
        "  def __init__(self,name = 'semantic_cache'):\n",
        "    self.client = chromadb.Client()\n",
        "    self.embed_fn = OpenAIEmbeddingFunction(\n",
        "      api_key=userdata.get('OPENAI_API_KEY'),\n",
        "      model_name=\"text-embedding-3-small\"\n",
        "    )\n",
        "    self.collection = self.client.get_or_create_collection(\n",
        "        name = name,\n",
        "        embedding_function=self.embed_fn,\n",
        "        metadata={'hnsw:space':'cosine'}\n",
        "    )\n",
        "  def get(self,query,threshold=0.20):\n",
        "    results = self.collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results = 1\n",
        "    )\n",
        "    # print(f'get results : {results}')\n",
        "\n",
        "    if results['distances'][0] and results['distances'][0][0] < threshold:\n",
        "      return results['metadatas'][0][0]['response']\n",
        "    return None\n",
        "  def set(self, query, response):\n",
        "    import uuid  # unique id 를 자동 생성\n",
        "    self.collection.add(\n",
        "        documents=[query],\n",
        "        metadatas=[{'response':response}],\n",
        "        ids=[str(uuid.uuid4())]\n",
        "    )\n",
        "cache = SemanticCache(name='test3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIdyPQaqezOv",
        "outputId": "7db31853-6188-40ac-8a62-01db648af38f"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "uuid.uuid4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av8A60-8fivi",
        "outputId": "d2165a7f-c609-4e1f-c242-ffae64e334c4"
      },
      "outputs": [],
      "source": [
        "# SemanticCache 사용\n",
        "questions = [\n",
        "    '대한민국의 수도는?',\n",
        "    '대한민국의 수도는?',  # 캐시 히트\n",
        "    '한국의 수도는?',      # 캐시 미스(다른 문자열)\n",
        "]\n",
        "for q in questions:\n",
        "  cached = cache.get(q)\n",
        "  if cached:\n",
        "    print(f'HIT : {q} - {cached}')\n",
        "  else:\n",
        "    response = call_llm(q)\n",
        "    cache.set(q,response)\n",
        "    print(f'MISS : {q} - {response}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inh1oKoZgk-_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "L1 메모리  내부메모리.. dictionary\n",
        "L2 메모리  벡터DB - 의미적 유사성\n",
        "L3 메모리  LLM호출\n",
        "'''\n",
        "\n",
        "class MulltiLevelCache:\n",
        "  def __init__(self) -> None:\n",
        "    self.l1_cach = SimpleCache()  # 메모리방식 dictionary   완전일치\n",
        "    self.l2_cach = SemanticCache() # ChoromaDB 벡터DB  유사도방식\n",
        "  def stats(self):\n",
        "    print(f'L1 catch: {self.l1_cach.cache}')\n",
        "  def get(self,key):\n",
        "    cached = self.l1_cach.get(key)\n",
        "    if cached:\n",
        "      print('L1 cache')\n",
        "      return cached\n",
        "    cached = self.l2_cach.get(key)\n",
        "    if cached:\n",
        "      print('L2 cache')\n",
        "      self.l1_cach.set(key,cached)\n",
        "      return cached\n",
        "    # LLM 호출\n",
        "    print('LLM')\n",
        "    response = call_llm(key)\n",
        "    self.l1_cach.set(key,response)\n",
        "    self.l2_cach.set(key,response)\n",
        "    return response\n",
        "mulltiLevelCache = MulltiLevelCache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "o5dSfLIM3YMl",
        "outputId": "500d4f50-6357-4370-fc78-0cbabad2ed5c"
      },
      "outputs": [],
      "source": [
        "mulltiLevelCache.get('america의  수도는')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12XcHasI5RAb",
        "outputId": "bbb32388-44e2-48e2-e48a-a76aaa087016"
      },
      "outputs": [],
      "source": [
        "mulltiLevelCache.stats()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
