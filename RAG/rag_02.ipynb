{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f667066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG + Cache + Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31007ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df180f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57e6cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict,Any\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "class RagSystem:\n",
    "    '''\n",
    "        ChromaDB + openai 임베딩\n",
    "    '''\n",
    "    def __init__(self,name='temp'):\n",
    "        self.client = chromadb.Client()\n",
    "        self.collection = self.client.get_or_create_collection(name = name)\n",
    "        self.embed_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "            api_key = openai.api_key,\n",
    "            model_name = 'text-embedding-3-small'\n",
    "        )\n",
    "        if len(self.collection.get()) == 0:\n",
    "            docs = [\n",
    "                ('대한민국 수도','서울입니다.'),\n",
    "                ('미국 수도','워싱턴 DC 입니다.'),\n",
    "                ('AI 정의','인간의 지능을 모방한 기술입니다.')\n",
    "            ]\n",
    "            for doc_id ,(title,text) in enumerate(docs):\n",
    "                self.collection.add(\n",
    "                    documents=[text],\n",
    "                    metadata = [{'title':title}],\n",
    "                    ids = [str(doc_id)],\n",
    "                    embedding_function = self.embed_fn\n",
    "                )\n",
    "    def query(self, question:str) -> str:\n",
    "        embedding = self.embed_fn(question)\n",
    "        results = self.collection.query(\n",
    "            query_embeddings = [embedding],\n",
    "            n_results=1\n",
    "        )\n",
    "        if results['documents'][0]:\n",
    "            doc_text = results['documents'][0][0]\n",
    "            return f'RAG 기반 답변 : {doc_text}'\n",
    "        else: # 문서에 없으면\n",
    "            client = OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model='gpt-5-nano',                    \n",
    "                messages=[{\n",
    "                    'role':'user',\n",
    "                    'content' : question\n",
    "                }] \n",
    "            )       \n",
    "            return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814195e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulltiLevelCache:\n",
    "  def __init__(self) -> None:\n",
    "    self.l1_cach = SimpleCache()  # 메모리방식 dictionary   완전일치\n",
    "    self.l2_cach = SemanticCache() # ChoromaDB 벡터DB  유사도방식\n",
    "  def stats(self):\n",
    "    print(f'L1 catch: {self.l1_cach.cache}')\n",
    "  def get(self,key):\n",
    "    cached = self.l1_cach.get(key)\n",
    "    if cached:\n",
    "      print('L1 cache')\n",
    "      return cached\n",
    "    cached = self.l2_cach.get(key)\n",
    "    if cached:\n",
    "      print('L2 cache')\n",
    "      self.l1_cach.set(key,cached)\n",
    "      return cached\n",
    "    # LLM 호출\n",
    "    print('LLM')\n",
    "    response = call_llm(key)\n",
    "    self.l1_cach.set(key,response)\n",
    "    self.l2_cach.set(key,response)\n",
    "    return response\n",
    "class GuardrailsSystem: # 사용자 방식\n",
    "    def __init__(self):\n",
    "        self.bad_words = ['욕설','비속어','나쁜말']\n",
    "    def validate_input(self, text:str):\n",
    "        if len(text.strip()) == 0:\n",
    "            return False, '입력이 비어 있습니다'\n",
    "        for b in self.bad_words:\n",
    "            if b in text:\n",
    "                return False, '입력에 허용되지 않는 단어가 포함되어 있습니다.'\n",
    "        return True, 'ok'\n",
    "    def validate_output(self, text:str):\n",
    "        for b in self.bad_words:\n",
    "            if b in text:\n",
    "                return False, '출력에 허용되지 않는 단어가 포함되어 있습니다.'\n",
    "        return True, 'ok'\n",
    "\n",
    "class LLMApplication:\n",
    "    def __init__(self):\n",
    "        self.rag = RagSystem()\n",
    "        self.cache = CacheSystem()\n",
    "        self.guardrails = GuardrailsSystem()\n",
    "    def query(self, question:str):\n",
    "        # 1.입력 검증\n",
    "        valid,msg = self.guardrails.validate_input(question)\n",
    "        if not valid:\n",
    "            return \"error\"\n",
    "        # 2.캐쉬 확인\n",
    "        cached  = self.cache.get(question)\n",
    "        if cached:\n",
    "            return cached\n",
    "        # 3.RAG 실행\n",
    "        response = self.rag.query(question)\n",
    "        # 4.출력 검증\n",
    "        valid,msg = self.guardrails.validate_input(response)\n",
    "        if not valid:\n",
    "            return \"error\"\n",
    "        self.cache.set(question,response)\n",
    "        return response       \n",
    "        \n",
    "# 실행.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
