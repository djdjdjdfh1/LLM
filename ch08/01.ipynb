{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d92387",
   "metadata": {},
   "source": [
    "### ìì—°ì–´ ê°ì„±ë¶„ì„\n",
    "- ê°ì„±ì‚¬ì „ ê¸°ë°˜ : ë¯¸ë¦¬ì •ì˜ëœ ê°ì„± ë‹¨ì–´ ì‚¬ì „ ì‚¬ìš©(ê·œì¹™ ê¸°ë°˜)\n",
    "    - TextBlob, AFINNm VADER\n",
    "- ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ : ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ í•™ìŠµ(í†µê³„ ê¸°ë°˜)\n",
    "    - TF-IDF ë²¡í„°í™”\n",
    "    - ì„ í˜•íšŒê·€\n",
    "    - ë¡œì§€ìŠ¤í‹±íšŒê·€\n",
    "    - F1 Score, Recison, Recall  -> classification report\n",
    "### ì‚¬ìš© ë°ì´í„°\n",
    "- NLTK ì˜í™” ë¦¬ë·°(2000ê°œ)\n",
    "- ë‹¤ìŒì˜í™”ë¦¬ë·°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c10d0c",
   "metadata": {},
   "source": [
    "#### ì•Œê³ ë¦¬ì¦˜\n",
    " - TextBlob     ì‚¬ì „ê¸°ë°˜ ê°ì„±ë¶„ì„\n",
    " - AFINN        ê°ì • ì ìˆ˜ ë§¤í•‘\n",
    " - VADER(Valence Aware Dictionary)  ì†Œì…œë¯¸ë””ì–´ ìµœì í™” ê°ì„±ë¶„ì„\n",
    " - TF-IDF       í…ìŠ¤íŠ¸ ë²¡í„°í™”\n",
    " - Multinomial Naive Bayes          í™•ë¥  ê¸°ë°˜ ë¶„ë¥˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob\n",
    "# ì´ ì˜í™”ëŠ” ì •ë§ ì¢‹ê³  ì¬ë¯¸ìˆë‹¤\n",
    "    # ì¢‹ë‹¤ +1(ê¸ì •)\n",
    "    # ì¬ë¯¸ìˆë‹¤ +1(ê¸ì •)\n",
    "    # +2 > 0 --> ê¸ì •(pos) ë¶„ë¥˜\n",
    "# Polarity(ê·¹ì„±ë„)  (ê¸ì •ë‹¨ì–´ìˆ˜ ê°œìˆ˜ - ë¶€ì •ë‹¨ì–´ ê°œìˆ˜) / ì „ì²´ ë‹¨ì–´ ê°œìˆ˜\n",
    "# -1.0 ~ +1.0\n",
    "# 0 ì¤‘ë¦½\n",
    "# Subjectivity(ì£¼ê´€ì„±) í‰ê°€ëŒ€ìƒ ë‹¨ì–´ ë¹„ìœ¨\n",
    "# 0.0 ~ 1.0\n",
    "# 0 : ê°ê´€ì    1 : ì£¼ê´€ì \n",
    "\n",
    "# ë¬¸ë§¥ë¬´ì‹œí•˜ê³  ë‹¨ì–´ ê·¹ì„±ë§Œ ê³ ë ¤\n",
    "# ì´ ì˜í™”ëŠ” ë‚˜ì˜ì§€ ì•Šë‹¤  -> ë‚˜ì˜ë‹¤(-)  ì•Šë‹¤(-) ë¡œ ì¸ì‹\n",
    "# ë‹¨ì  : ë¹ ë¥¸ ì†ë„, í•™ìŠµ ë¶ˆí•„ìš”\n",
    "# ì‚¬ìš© : ì‹¤ì‹œê°„ ê°ì„±ë¶„ì„,  ìŠ¤íŠ¸ë¦¬ë°ë°ì´í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word\n",
    "text = \"TextBlob is amazingly simple to use. What a wonderful library for NLP!\"\n",
    "blob =  TextBlob(text)\n",
    "print(blob.sentences)\n",
    "print(blob.words)\n",
    "print(blob.tags)\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "print(blob.noun_phrases)\n",
    "print('-'*100)\n",
    "# ê°ì„±ë¶„ì„\n",
    "blob.sentiment\n",
    "print(f'polarity : {blob.sentiment.polarity}')\n",
    "print(f'subjectivity : {blob.sentiment.subjectivity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFINN(Lexicon-Based)  ê°ì„±ì‚¬ì „\n",
    "# ê° ë‹¨ì–´ì˜ -5 ~ +5ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³  í•©ì‚°\n",
    "# ì´ ì˜í™”ëŠ” ì¢‹ì§€ë§Œ ì¢‹ì§€ ì•Šì€ ë¶€ë¶„ë„ ìˆë‹¤\n",
    "    # ì¢‹ë‹¤ +3  ì¢‹ë‹¤ +3  ë‚˜ì˜ë‹¤ -3  =  +3 > 0 ê¸ì •\n",
    "# score = sum(word_sentiment_value)\n",
    "# ë¶„ë¥˜ê·œì¹™ score > 0 ê¸ì •  score < 0 ë¶€ì •  \n",
    "# ì´ëª¨í‹°ì½˜ì§€ì›\n",
    "# ê°•ë„í‘œí˜„ ì¸ì‹ very , really ë“±\n",
    "\n",
    "# ê°•ì¡° ìˆ˜ì •ì(intensifiers)\n",
    "    # ë§¤ìš°ì¢‹ë‹¤ = 1.5X(ì¢‹ë‹¤ì˜ ì ìˆ˜)\n",
    "\n",
    "# AFINN vs TextBlob\n",
    "# AFINN : ë” ì •í™•í•œ ì ìˆ˜ ë§¤í•‘\n",
    "# TextBlob : ë” ì¼ë°˜ì ì¸ ì ‘ê·¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "af = Afinn()\n",
    "text1 = \"TextBlob is amazingly simple to use\"\n",
    "text2 = \"What a wonderful library for NLP!\"\n",
    "score1 = af.score(text1)\n",
    "score2 = af.score(text2)\n",
    "score1, score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afb4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER  ì‡¼ì„¤ ë¯¸ë””ì–´ í…ìŠ¤íŠ¸ì— ìµœì í™”\n",
    "# ì´ ì˜í™”ëŠ” ì •ë§ì •ë§ í›Œë¥­í•´!!!\n",
    "# í›Œë¥­í•˜ë‹¤ (ê¸°ë³¸) + 0.7   ì •ë§ì •ë§ (ê°•ì¡°)x1.5\n",
    "# !!! (ë¬¸ì¥ë¶€í˜¸ê°•ì¡°) x1.2\n",
    "# 4ê°œì˜ ê°ì • ì§€ìˆ˜\n",
    "    # positive ê¸ì • í™•ë¥  0 ~ 1\n",
    "    # nagative ë¶€ì • í™•ë¥ \n",
    "    # neutral ì¤‘ë¦½ í™•ë¥ \n",
    "    # compound ì¢…í•©ì ìˆ˜ -1 ~ 1\n",
    "# score = compound_score / sqrt(compound_score**2 + 0.0625)\n",
    "# score >=0.05 ê¸ì •\n",
    "# score <=-0.05 ë¶€ì •  \n",
    "# ê·¸ ì‚¬ì´ëŠ” ì¤‘ë¦½\n",
    "# ëŒ€ì†Œë¬¸ì êµ¬ë¶„  AMAZING amazing ë‹¤ë¥¸ ì ìˆ˜\n",
    "# :) ê¸ì •    :-(  ë¶€ì •\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a54db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adceed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyer = SentimentIntensityAnalyzer()\n",
    "sentences = [\n",
    "    \"I love this product! It's absolutely amazing ğŸ˜\",\n",
    "    \"This is the worst movie I've ever seen...\",\n",
    "    \"The food was okay, not great but not bad either.\",\n",
    "    \"Iâ€™m REALLY happy with the results!!!\",\n",
    "    \"Not good at all. Iâ€™m disappointed.\",\n",
    "]\n",
    "for s in sentences:\n",
    "    scores = analyer.polarity_scores(s)\n",
    "    print(f'ë¬¸ì¥ : {s}')\n",
    "    print(f'ì ìˆ˜ : {scores}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from textblob import TextBlob\n",
    "from afinn import Afinn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# nltk ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# ì˜í™” ë¦¬ë·° ë°ì´í„° ë¡œë“œ\n",
    "fileids = movie_reviews.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a601141",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [movie_reviews.raw(fileid) for fileid in  fileids[:50]] + \\\n",
    "        [movie_reviews.raw(fileid) for fileid in  fileids[-50:]]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in  fileids[:50]] + \\\n",
    "        [movie_reviews.categories(fileid)[0] for fileid in  fileids[-50:]]\n",
    "len(reviews), categories.count('pos'), categories.count('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 TextBlob\n",
    "def sentiment_textblob(docs):\n",
    "    return ['pos' if TextBlob(doc).sentiment.polarity>0 else 'neg' for doc in docs ]\n",
    "predictions_textblob = sentiment_textblob(reviews)\n",
    "accuracy_textblob = accuracy_score(categories,predictions_textblob)\n",
    "print(f'ì •í™•ë„ : {accuracy_textblob:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89461d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2  AFINN\n",
    "def sentiment_afinn(docs):\n",
    "    afn = Afinn(emoticons=True)\n",
    "    return [ 'pos' if afn.score(doc) > 0 else 'neg'  for doc in docs]\n",
    "predictions = sentiment_afinn(reviews)\n",
    "accuracy = accuracy_score(categories,predictions)\n",
    "print(f'ì •í™•ë„ : {accuracy:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e895b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 VADER\n",
    "def sentiment_vader(docs):\n",
    "    analyer = SentimentIntensityAnalyzer()\n",
    "    return [ 'pos' if analyer.polarity_scores(doc)['compound'] > 0 else 'neg'  for doc in docs]\n",
    "predictions = sentiment_vader(reviews)\n",
    "accuracy = accuracy_score(categories,predictions)\n",
    "print(f'ì •í™•ë„ : {accuracy:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„±ë¶„ì„ \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ\n",
    "\n",
    "# ë² ì´ì¦ˆ ì •ë¦¬\n",
    "# \"ì¢‹ë‹¤\" ë‹¨ì–´ë¥¼ ë³¸í›„ ì´ ë¦¬ë·°ê°€ ê¸ì •ì¼ í™•ë¥ \n",
    "# p(ê¸ì • | \"ì¢‹ë‹¤\") = p(\"ì¢‹ë‹¤\" | ê¸ì •) x p(ê¸ì •) / p('ì¢‹ë‹¤')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¶„í• \n",
    "dataset = train_test_split(reviews, categories, test_size=0.2,random_state=42, stratify=categories)\n",
    "len(dataset[0]),len(dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3268bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf ë²¡í„°í™”\n",
    "vectorizer =  TfidfVectorizer(max_features=1000)\n",
    "x_train = vectorizer.fit_transform(dataset[0])\n",
    "x_test = vectorizer.transform(dataset[1])\n",
    "\n",
    "y_train = dataset[2]\n",
    "y_test = dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. mnb\n",
    "mnb_clf = MultinomialNB()\n",
    "mnb_clf.fit(x_train,y_train)\n",
    "predict = mnb_clf.predict(x_test)\n",
    "print( classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logisticregression\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(x_train,y_train)\n",
    "predict = lr_clf.predict(x_test)\n",
    "print( classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6831a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥í–¥ìƒ\n",
    "# ì†Œë¬¸ìë³€í™˜ - ì—°ì†ëœ ë¬¸ìì—´ì¤‘ì— 3ê¸€ì ì´ìƒ - ì–´ê°„ì¶”ì¶œ(í˜•íƒœì†Œë¶„ì„) - ë¶ˆìš©ì–´ ì œê±°\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4709df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer =  RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    porter = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [porter.stem(token) for token in tokens if token not in stop_words]\n",
    "vector = TfidfVectorizer(\n",
    "    tokenizer  = custom_tokenizer\n",
    "    ,max_features=1000\n",
    "    ,min_df=5\n",
    "    ,max_df=0.5\n",
    "    ,token_pattern = r\"[\\w']{3,}\"\n",
    "    ,ngram_range = (1,1)\n",
    ")\n",
    "x_train = vector.fit_transform(dataset[0])\n",
    "x_test = vector.transform(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e347073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):    \n",
    "    model.fit(x_train,y_train)\n",
    "    predict = model.predict(x_test)\n",
    "    print( classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435739d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21438afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° - í† í°í™” & ì •ìˆ˜ ì¸ì½”ë”© - ì‹œí€€ìŠ¤ íŒ¨ë”©\n",
    "# baseline ì¼ë°˜ì‹ ê²½ë§ Dense\n",
    "# Simple RNN ê¸°ìš¸ê¸°ì†Œì‹¤ë¬¸ì œ\n",
    "# Bidirectional LSTM ì–‘ë°©í–¥ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a175a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜\n",
    "# Word Embedding ë‹¨ì–´ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜\n",
    "# Sequence Padding ê¸¸ì´ê°€ ë‹¤ë¥¸ ë¬¸ì¥ì„ ê°™ì€ í¬ê¸°ë¡œ ê³ ì •\n",
    "# Simple RNN\n",
    "# LSTM          RNN ì˜ ê²½ì‚¬ì†Œì‹¤ë¬¸ì œ í•´ê²°\n",
    "# Bidiractional LSTM ì–‘ë°©í–¥ ì»¨í…ì¸  í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0704b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í°í™” : ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •\n",
    "# í† í¬ë‚˜ì´ì ¸ 3ë‹¨ê³„\n",
    "    # 1. fit_on_text(texts)  ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•´ì„œ ë”•ì…”ë„ˆë¦¬\n",
    "    # 2. texts_to_sequence(texts) ê° ë¬¸ì„œë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ ë³€í™˜\n",
    "    # 3. pad_sequence() ê¸¸ì´ ì •ê·œí™”(ê°™ì€ ê¸¸ì´)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab96f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë”¥ëŸ¬ë‹ì—ì„œ ì›Œë“œ ì„ë² ë”© ë ˆì´ì–´ : ê° ë‹¨ì–´ë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ ì‹¤ìˆ˜ ë²¡í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = [\n",
    "    \"this movie is great and wonderful\",\n",
    "    \"bad movie with poor acting\",\n",
    "    \"great movie absolutely wonderful\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb10710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì´í† ì¹˜ ë²„ì „\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ë‹¨ì–´ ë¶„í•  ë° ë¹ˆë„ ê³„ì‚°\n",
    "all_words=[]\n",
    "for i in [review.split() for review in sample_reviews]:\n",
    "    all_words.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95998f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ì–´ë¹ˆë„\n",
    "word_freq = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620471bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenizer êµ¬í˜„\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, num_words=10, oov_token='UNK'):\n",
    "        self.num_words = num_words\n",
    "        self.oov_token = oov_token\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "    def fit_on_texts(self, texts):\n",
    "        '''ë¬¸ì¥ì„ ë‹¨ì–´ì¸ë±ìŠ¤ë¡œ ë³€í™˜'''\n",
    "        all_words = []\n",
    "        for i in [review.split() for review in sample_reviews]:\n",
    "            all_words.extend(i)\n",
    "        word_freq = Counter(all_words)\n",
    "        # ë¹ˆë„ ë†’ì€ ìˆœì„œë¡œ ì¸ë±ìŠ¤ ë¶€ì—¬\n",
    "        # oov í† í°ì„ 1ë¡œ ì„¤ì •\n",
    "        self.word_index[self.oov_token] = 1\n",
    "        self.index_word[1] = self.oov_token\n",
    "        idx = 2\n",
    "        for word, _ in word_freq.most_common(self.num_words-1):\n",
    "            self.word_index[word] = idx\n",
    "            self.index_word[idx] = word\n",
    "            idx += 1\n",
    "    def texts_to_sequences(self, texts):\n",
    "        '''í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜'''\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            seq = []\n",
    "            for word in text:\n",
    "                # ë‹¨ì–´ê°€ vocabularyì— ìˆìœ¼ë©´ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©, ì—†ìœ¼ë©´ oov\n",
    "                word_index = self.word_index.get(word, 1)\n",
    "                seq.append(word_index)\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "\n",
    "# Tokenizer ìƒì„± ë° í•™ìŠµ\n",
    "tokenizer = SimpleTokenizer(num_words=10, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(sample_reviews)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aead7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "sequences = tokenizer.texts_to_sequences(sample_reviews)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "82fb1c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this movie is great and wonderful',\n",
       " 'bad movie with poor acting',\n",
       " 'great movie absolutely wonderful']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17246def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒ¨ë”© êµ¬í˜„ - ë¬¸ìì—´ì˜ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶˜ë‹¤"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
