{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0a5298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 영어 번역 데이터셋\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# 실제프로젝트  AI hub 기타등등..\n",
    "korean_sentences = [\n",
    "    \"안녕하세요\",\n",
    "    \"오늘 날씨가 좋아요\",\n",
    "    \"저는 학생입니다\",\n",
    "    \"이것은 사과입니다\",\n",
    "    \"고양이가 자고 있어요\",\n",
    "    \"내일 비가 올까요\",\n",
    "    \"저는 커피를 좋아해요\",\n",
    "    \"그는 의사입니다\",\n",
    "    \"이 책은 재미있어요\",\n",
    "    \"우리는 친구예요\"\n",
    "]\n",
    "\n",
    "english_sentences = [\n",
    "    \"hello\",\n",
    "    \"the weather is nice today\",\n",
    "    \"i am a student\",\n",
    "    \"this is an apple\",\n",
    "    \"the cat is sleeping\",\n",
    "    \"will it rain tomorrow\",\n",
    "    \"i like coffee\",\n",
    "    \"he is a doctor\",\n",
    "    \"this book is interesting\",\n",
    "    \"we are friends\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7e7f93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"e'\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트 전처리\n",
    "# unicode 정규화, 특수문자처리\n",
    "# NFD Normalization Form Decomposition -> 분해가능한 모든 문자를 분해한다\n",
    "\n",
    "import unicodedata\n",
    "unicodedata.normalize('NFD',\"e'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616299f",
   "metadata": {},
   "source": [
    "$\\acute{e}$   \n",
    "\n",
    "단일문자 U+00E9\n",
    "\n",
    "$e$ + $\\acute{}$\n",
    "\n",
    "$e$ + 악센트\n",
    "\n",
    "###### 같은 문장처럼 보이지만 내부적으로 다른 바이트조합을 갖는 경우 -> 동일한 내부 표현으로 바꿔주는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e46c44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 전처리 함수\n",
    "def preprocess_stentence(sentence, is_korean = False):\n",
    "    '''\n",
    "    Unicode정규화, 특수문자처리\n",
    "    Args : \n",
    "        sentence: 원본\n",
    "        is_korean : 한국어 여부\n",
    "    return:\n",
    "        전처리된 문장\n",
    "    '''\n",
    "    sentence = unicodedata.normalize('NFD',sentence)\n",
    "    if not is_korean:\n",
    "        sentence = sentence.lower()\n",
    "    sentence = sentence.strip()\n",
    "    # 정규식으로 특수문자 전후에 공백 추가\n",
    "    # 안녕하세요! -> 안녕하세요 !\n",
    "    # r\"([?.!,])\"   문자중에 ?.!, 나오면 문자를 그룹으로 캡쳐\n",
    "    # r\" \\1 \" 캡처한 문자( \\1 ) 앞뒤로 공백을 하나씩 넣는다\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]', \" \", sentence)\n",
    "    # 시작 종료토큰 추가\n",
    "    sentence = '<start>' + sentence + '<end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4364ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_processed = [preprocess_stentence(kor) for kor in korean_sentences]\n",
    "english_processed = [preprocess_stentence(eng) for eng in english_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a19cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 정수 시퀀스로 변환\n",
    "# 3. 토크나이져\n",
    "def create_tokenizer(sentence):\n",
    "    '''\n",
    "    단어를 정수 인덱스로 변환\n",
    "    Vocabulary구축, word to index mapping\n",
    "    Args:\n",
    "        sentence : 문장 리스트\n",
    "    Returns:\n",
    "        Tokenizer : keras Tokenizer 객체\n",
    "    '''\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters='', # 필터 비활성화\n",
    "        oov_token='<unk>', # out of vocabulary \n",
    "    )\n",
    "    # 모든 문장으로 단어사전 구축\n",
    "    tokenizer.fit_on_texts(sentence)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "307c2a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 사전 크기 : 25\n",
      "영어 사전 크기 : 30\n"
     ]
    }
   ],
   "source": [
    "# 한국어, 영어 각가의 토크나이져 생성\n",
    "korean_tokenizer = create_tokenizer(korean_processed)\n",
    "english_tokenizer = create_tokenizer(english_processed)\n",
    "\n",
    "# 단어 사전 \n",
    "korean_vocab_size = len(korean_tokenizer.word_index) + 1\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "print(f'한국어 사전 크기 : {korean_vocab_size}')\n",
    "print(f'영어 사전 크기 : {english_vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbfcd04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 정수시퀀스 변경\n",
    "def encode_sentences(tokenizer, sentences, maxlen):\n",
    "    '''\n",
    "    문장을 고정길이의 정수 시퀀스로 변환\n",
    "    padding : 짧은 문장을 동일 길이로 맞추는\n",
    "    Args: \n",
    "        tokenizer \n",
    "        sentences : 문장리스트\n",
    "        maxlen : 가장 긴 문장길이\n",
    "    Returns:\n",
    "        패딩된 정수 시퀀스배열  tf의 pad_sequence\n",
    "    '''\n",
    "    sequences = tokenizer.texts_to_sequences(sentences) # [i love you] [1, 5, 7]\n",
    "    # 길이 맞추기 (패딩추가)\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences,\n",
    "        maxlen=maxlen,\n",
    "        padding='post'\n",
    "    )\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a982998",
   "metadata": {},
   "source": [
    "- 딥러닝 RNN/LSTM/GUR  전통적인 시계열 처리 기법\n",
    "    - pre\n",
    "    - RNN 앞에서부터 읽어 -> 실제 단어가 뒤쪽에 몰려있다\n",
    "- Transformer 계열은 post\n",
    "    - 위치정보 -> 앞 뒤 순서를 그대로 유지해야 자연스럽다 -> 뒤쪽 패딩은.. masking 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db3773c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시퀀스의 최대길이\n",
      "한국어: 3\n",
      "영어: 5\n",
      "인코딩 첫번째 문장\n",
      "한국어 인코딩 문장: [3 0 0]\n",
      "영어 인코딩 문장: [7 0 0 0 0]\n",
      "한국어 문장: <start>안녕하세요<end>\n",
      "영어 문장: <start>hello<end>\n",
      "데이터셋 준비 완료\n",
      "전체 샘플 수 : 10\n",
      "배치 크기 : 2\n",
      "배치 수 : 5\n"
     ]
    }
   ],
   "source": [
    "# 최대 시퀀스의 길이 결정(가장 긴 문장기준)\n",
    "max_korean_len = max(len(s.split()) for s in korean_processed)\n",
    "max_english_len = max(len(s.split()) for s in english_processed)\n",
    "print(f'시퀀스의 최대길이')\n",
    "print(f'한국어: {max_korean_len}')\n",
    "print(f'영어: {max_english_len}')\n",
    "# 인코딩 수행\n",
    "korean_tensor = encode_sentences(korean_tokenizer, korean_processed, max_korean_len)\n",
    "english_tensor = encode_sentences(english_tokenizer, english_processed, max_english_len)\n",
    "print(f'인코딩 첫번째 문장')\n",
    "print(f'한국어 인코딩 문장: {korean_tensor[0]}')\n",
    "print(f'영어 인코딩 문장: {english_tensor[0]}')\n",
    "print(f'한국어 문장: {korean_processed[0]}')\n",
    "print(f'영어 문장: {english_processed[0]}')\n",
    "\n",
    "BUFFER_SIZE = len(korean_tensor)\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# tensorflow Dataset 객체\n",
    "# 배치처리 와 셔플\n",
    "dataset = tf.data.Dataset.from_tensor_slices( (korean_tensor, english_tensor) )\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(f'데이터셋 준비 완료')\n",
    "print(f'전체 샘플 수 : {len(korean_tensor)}')\n",
    "print(f'배치 크기 : {BATCH_SIZE}')\n",
    "print(f'배치 수 : {len(korean_tensor) // BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f074448",
   "metadata": {},
   "source": [
    "-- step2 --\n",
    "\n",
    "seq2seq 구조\n",
    "```\n",
    "Encoder\n",
    "    입력문장 -> Embedding -> LSTM + Hidden States\n",
    "Decoder\n",
    "    <start>토큰 -> Embedding -> LSTM + context -> 단어 예측\n",
    "    예측단어 -> 다음입력 -> 반복(<end> 나올때 까지)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0bd26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 클래스\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    입력문장을 고차원 벡터로 압축\n",
    "    Embedding -> LSTM -> Hidden States 출력\n",
    "    구조 :\n",
    "        입력(정수시퀀스) -> Embedding -> LSTM -> 모든 타입스텝의 출력\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        '''\n",
    "        Args:\n",
    "            vocab_size : 단어사전 크기(임베딩 테이블 크기)\n",
    "            embedding_dim : 임베딩 벡터 차원\n",
    "            enc_units : LMS 유닛수(hidden_state 차원)\n",
    "            batch_size : 배치크기\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        # Embedding Layer : 정수 --> 밀집벡터(Dense Vector) : 작은차원 대부분의 값이 0이 아님, 연속된 실수값 공간효율성\n",
    "        # 학습과정을 통해 단어 간 의미적 관계학습 -> 유사 단어는 가까운 벡터 공간에 위치\n",
    "        # 단점 : 훈련필요(사전학습 또는 임베딩을 학습)\n",
    "\n",
    "        # 희소벡터 : 대부분의 값이 0(대표적인 원-핫벡터) 차원이 크다, 단어간 충돌이없다 단어간 유사도 표현못함\n",
    "        # 희소벡터 : 규칙기반(원핫 Vow) - 초창기 자연어모델\n",
    "        # 밀집벡터 : 학습기반(Word2Vec Glove Embedding)\n",
    "        \n",
    "        # mask_zero = True  패딩에 마스크처리를 해서 모델이 해석하지 않게 한다\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, mask_zero=True\n",
    "        )\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            enc_units,\n",
    "            return_sequence = True, #[batch, seq_len, units]\n",
    "            return_state=True, #(output, h, c)\n",
    "            recurrent_initializer = 'glorot_uniform', # 가중치 초기화    \n",
    "        )\n",
    "    def call(self, x, hidden):\n",
    "        '''\n",
    "        입력 시퀀스를 처리해서 hidden states 생성\n",
    "        Args:\n",
    "            x : 입력 시퀀스 [ baatch_size, seq_len ]\n",
    "            hidden : 초기 hidden state(첫 호출시 0벡터)\n",
    "        Returns:\n",
    "            output: 모든 타임스탬프의 출력 [batch, seq_len, enc_units]\n",
    "            state_h: 마지막 hidden state [batch, enc_units]\n",
    "            state_c: 마지막 cell state [batch, enc_units]\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ca3f721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "호출 100\n"
     ]
    }
   ],
   "source": [
    "class ABC():\n",
    "    def __call__(self, data):\n",
    "        self.call(data)\n",
    "    \n",
    "    def call(self, data):\n",
    "        print('호출', data)\n",
    "\n",
    "a=ABC()\n",
    "a(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
