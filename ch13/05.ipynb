{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a85c83",
   "metadata": {},
   "source": [
    "Attention Mechanism\n",
    "\n",
    "고양이가 잔다 - > the cat sleeps\n",
    "\n",
    "입력 : \"고양이가 잔다\"\n",
    "\n",
    "[전처리] -> <start> 고양이가 잔다 <end>\n",
    "\n",
    "[ENCODER] -> 각 단어마다 hidden state 생성\n",
    "\n",
    "[ATTENTION] -> 어디에 집중할지 계산\n",
    "\n",
    "[DECODER] -> 한 단어씩 생성\n",
    "\n",
    "출력 : the cat sleeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793485e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본\n",
    "input = '고양이가 자요'\n",
    "# 1단계 전처리\n",
    "input = f'<start> {input} <end>'\n",
    "# 2단계 토큰화(단어 -> 숫자)\n",
    "word_dict = {\n",
    "    '<pad>' : 0,\n",
    "    '<start>' : 1,\n",
    "    '<end>' : 2,\n",
    "    '고양이가' : 3,\n",
    "    '자요' : 4,\n",
    "}\n",
    "\n",
    "# 최대길이 5\n",
    "정수_시퀀스 = [1,3,4,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 : EMBEDDING(숫자 -> 벡터)\n",
    "# 임베딩레이어 ============= 256차원\n",
    "정수_시퀀스 = [1,3,4,2,0]\n",
    "# Embedding....\n",
    "임베딩_결과 = [\n",
    "    [0.2, 0.5, ..., 0.3], #1 (<start>)의 벡터 (256)차원\n",
    "    [0.2, 0.5, ..., 0.3], #3 (<start>)의 벡터 (256)차원\n",
    "    [0.2, 0.5, ..., 0.3], #4 (<start>)의 벡터 (256)차원\n",
    "    [0.2, 0.5, ..., 0.3], #2 (<start>)의 벡터 (256)차원\n",
    "    [0.2, 0.5, ..., 0.3], #0 (<start>)의 벡터 (256)차원\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072719b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "# STEP 2 : ENCODER(벡터 -> Hidden States)\n",
    "# LSTM Encoder\n",
    "# 각 타임스텝마다 hidden state 생성\n",
    "\n",
    "# 초기상태\n",
    "h0 = [0,0,...,0] # 512차원 영벡터\n",
    "c0 = [0,0,...,0] # 512차원 영벡터\n",
    "\n",
    "# ================= 타임스텝 1 : <start> 처리 ================\n",
    "입력_t1 = [0.2,0.5, ..., 0.3] #  <start> 의 임베딩\n",
    "h1, c1 = LSTM(입력_t1, h0, c0)\n",
    "# h1 = [0.1, ......] 512차원\n",
    "\n",
    "# ================= 타임스템 2 : 고양이가 처리 ================\n",
    "입력_t2 = [0.2,0.5, ..., 0.3] # 고양이가 의 임베딩\n",
    "h2, c2 = LSTM(입력_t2, h1, c1)\n",
    "\n",
    "# ================= 타임스템 3 : 자요 처리 ================\n",
    "입력_t3 = [0.2,0.5, ..., 0.3] # 자요 의 임베딩\n",
    "h3, c3 = LSTM(입력_t3, h2, c2)\n",
    "\n",
    "# ================= 타임스템 4 : <end> 처리 ================\n",
    "입력_t4 = [0.2,0.5, ..., 0.3] # <end> 의 임베딩\n",
    "h4, c4 = LSTM(입력_t4, h3, c3)\n",
    "\n",
    "# ================= 타임스템 5 : 패딩 (무시) ================\n",
    "입력_t5 = [0.0,0.0, ..., 0.0] \n",
    "h5, c5 = LSTM(입력_t5, h4, c4)\n",
    "# h5는 사용 안함(마스킹)\n",
    "\n",
    "# ================= 인코더의 출력 =================\n",
    "encoder_output = [h1,h2,h3,h4,h5] # 모든 hideen state\n",
    "# 형태 [1,5,512]   [배치, 시퀀스, units]\n",
    "encoder_final_h = h4 # 마지막 hidden state(디코더 초기화용)\n",
    "encoder_final_c = c4 # 마지막 cell state\n",
    "\n",
    "# ================== 정리 ===================\n",
    "# <start>  ---> h1\n",
    "# 고양이가  ---> h2   \"고양이\" 정보를 압축\n",
    "# 자요     ---> h3   \"자요\" 정보를 압축\n",
    "# <end>    ---> h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bdde8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 : Decoder의 시작 (첫 단어 생성)\n",
    "# ============== 디코더 초기 상태 ===============\n",
    "decoder_h = h4  # 인코더의 마지막 hidden state\n",
    "decoder_c = c5\n",
    "\n",
    "# ============= 첫번째 입력 : <start> 토큰 ============\n",
    "decoder_input = [1]  # <start> 의 인덱스\n",
    "decoder_input_embedding = [0.3, 0.1, ..., 0.2 ] #256차원\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 : ATTENTION 계산 (첫 번쨰 단어)\n",
    "#### THE 를 생성하기 위해서 어디를 봐야 하나? ####\n",
    "# query 준비\n",
    "query = decoder_h # 512차원\n",
    "# 의미 : 지금 뭘 번역하려고 하는가?\n",
    "\n",
    "# key 준비\n",
    "keys = [h1, h2, h3, h4] # 패딩을 제외한 인코더 출력\n",
    "# 의미 : 각 입력 단어의 정보\n",
    "\n",
    "# value 준비(keys와 동일)\n",
    "values = [h1, h2, h3, h4] #패딩을 제외한 인코더 출력\n",
    "\n",
    "# 1단계 score 계산\n",
    "# W1 W2 V는 학습된 가중치 행렬\n",
    "# h1 <start> 에 대한 score 계산\n",
    "# h2 고양이가 에 대한 score 계산\n",
    "# h3 자요 에 대한 score 계산\n",
    "# h4 <end> 에 대한 score 계산\n",
    "scores = [ 0.3,0.8,0.2,0.1]\n",
    "\n",
    "# 2단계 Attention Weights 계산\n",
    "# softmax 확률 분포 exp(scores)\n",
    "exp_scores = [exp(0.3, exp(0.8)), ...] = [1.45, 2.23, 1.22, 1.11]\n",
    "# sum\n",
    "sum_exp = 1.45 + 2.23 + 1.22 + 1.11 = 5.91\n",
    "# 정규화\n",
    "attention_weight = [\n",
    "    1.35 / 5.91 = 0.23, # <start>\n",
    "    2.23 / 5.91 = 0.2, # 고양이가\n",
    "    1.22 / 5.91 = 0.41, # 자요\n",
    "    1.11 / 5.91 = 0.52, # <end> ---> 가중치가 가장 높음\n",
    "]\n",
    "\n",
    "# 3단계 context vector 계산\n",
    "# 가중합으로 인코더 정보 통합\n",
    "context_vector = [\n",
    "    0.23 x h1 + , # <start>의 정보 23%\n",
    "    0.2 x h2 + , # 고양이가 정보 20%\n",
    "    0.41, # 자요 정보 41%\n",
    "    0.52, # <end> 정보 52% ---> 가장 많이 참조\n",
    "]\n",
    "\n",
    "# 결과 512차원\n",
    "\n",
    "# 입력단어  score   Attention weight    기여도\n",
    "# ----      ----    -----              end에 대한 기여도가 가장 높게 나오고 ----> 집중\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5 : DECODER 실행 첫 단어 생성\n",
    "# context vector 와 입력을 결합해서 단어 생성\n",
    "\n",
    "# 1단게 입력과 context 결합\n",
    "decoder_input_emb = [0.2, ... , 0.2] # <start>의 임베딩(256차원)\n",
    "context_vector = [0.32, ... , 0.22] \n",
    "\n",
    "# concatenate 이어붙이기\n",
    "combined_input = [\n",
    "    0.3,0.2,...,0.2, # 임베딩 256\n",
    "    0.3,0.2,...,0.2, # context 512\n",
    "]\n",
    "# 형태: 256 + 512 768차원\n",
    "\n",
    "# --- 2단계 LSTM 처리\n",
    "decoder_output, new_h, new_c = LSTM(\n",
    "    combined_input,\n",
    "    decoder_h,\n",
    "    decoder_c,\n",
    ") \n",
    "#  ------- 3단계 : 단어 확률 분포 생성\n",
    "# Dense layer : 512 + vocab_size(예 50)\n",
    "logits = Dense(decoder_output)\n",
    "# logis = [-0.5, 0.2, ...] 50개의 단어 점수\n",
    "\n",
    "# softmax로 확률변환\n",
    "probailities = softmax(logits)\n",
    "# probailities = [0.0, 0.42, 0.52, ...]  인덱스 1 (the)가 가장 높게\n",
    "\n",
    "# 4단계 ===== 가장 높은 확률의 단어 선택\n",
    "predict_di = argmax(probailities) # 1\n",
    "\n",
    "# 단어 사전에서 lookup\n",
    "# 영어_단어_사전 = {\n",
    "\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
